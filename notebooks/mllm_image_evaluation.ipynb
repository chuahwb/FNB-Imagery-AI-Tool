{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chuahwb/FNB-Imagery-AI-Tool/blob/main/notebooks/mllm_image_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V74QgIDCNm4v",
        "outputId": "69757b67-3646-41cd-e1d1-14b30c52fefb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chuahwb/FNB-Imagery-AI-Tool.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDvdz8iTO3VZ",
        "outputId": "9c1d1686-f9dc-47d7-b64d-fc11359b172f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FNB-Imagery-AI-Tool'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 32 (delta 10), reused 4 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (32/32), 44.48 KiB | 3.18 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V9VtwYyPQFMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "IPython Notebook for Phase 2: Evaluating Multimodal LLMs for F&B Image Recreation\n",
        "\n",
        "This notebook connects to OpenRouter, processes local images, sends them with\n",
        "prompts to selected multimodal LLMs, and retrieves structured descriptions\n",
        "using the 'instructor' library for comparison.\n",
        "\"\"\"\n",
        "\n",
        "# @title Setup: Install Libraries and Import Modules\n",
        "# Install necessary libraries\n",
        "!pip install instructor openai python-dotenv pillow pandas tqdm -q\n",
        "\n",
        "import os\n",
        "import base64\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from typing import List, Optional\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from dotenv import load_dotenv\n",
        "import time\n",
        "\n",
        "# @title Configure API Key and OpenRouter Client\n",
        "\n",
        "# --- IMPORTANT ---\n",
        "# Set your OpenRouter API key.\n",
        "# Option 1: Create a .env file in the same directory as this notebook\n",
        "#           with the line: OPENROUTER_API_KEY=\"your-key-here\"\n",
        "# Option 2: Set it as an environment variable in your system.\n",
        "# Option 3: Replace os.getenv(\"OPENROUTER_API_KEY\") below with your actual key string\n",
        "#           (less secure, not recommended for shared notebooks).\n",
        "load_dotenv()\n",
        "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "    print(\"⚠️ OpenRouter API Key not found.\")\n",
        "    print(\"Please set the OPENROUTER_API_KEY environment variable or in a .env file.\")\n",
        "    # You might want to raise an error or use input() here in a real script\n",
        "    # OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \")\n",
        "\n",
        "\n",
        "# Configure the Instructor client to use OpenRouter\n",
        "# Patch the OpenAI client to add structured response capabilities\n",
        "client = instructor.patch(\n",
        "    OpenAI(\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        api_key=OPENROUTER_API_KEY,\n",
        "        default_headers={ # Optional, but good practice for OpenRouter\n",
        "            \"HTTP-Referer\": \"http://localhost:8888\", # Replace with your app URL if deployed\n",
        "            \"X-Title\": \"F&B Image Eval\", # Replace with your app name\n",
        "        },\n",
        "        timeout=600 # Increase timeout for potentially long image processing\n",
        "    ),\n",
        "    mode=instructor.Mode.MD_JSON # Use Markdown JSON mode for better compatibility\n",
        ")\n",
        "\n",
        "print(\"✅ OpenAI client patched with Instructor and configured for OpenRouter.\")\n",
        "\n",
        "# @title Define Pydantic Model for Structured Description\n",
        "# This model mirrors the 8 points requested in the prompts\n",
        "\n",
        "class FnbImageDescription(BaseModel):\n",
        "    \"\"\"Structured description of an F&B social media image.\"\"\"\n",
        "    primary_subject: str = Field(..., description=\"Detailed description of the main food, drink, person, or element, including ingredients, preparation, presentation, and actions.\")\n",
        "    composition_framing: str = Field(..., description=\"Description of layout (e.g., centered, rule of thirds), camera angle (e.g., eye-level, overhead), and framing (e.g., close-up, medium shot).\")\n",
        "    background_environment: str = Field(..., description=\"Details of the setting, surfaces, other objects, and depth of field (e.g., blurred background).\")\n",
        "    lighting_color: str = Field(..., description=\"Description of light source, style (e.g., natural, studio), direction, shadows, highlights, dominant colors, and temperature.\")\n",
        "    texture_materials: str = Field(..., description=\"Specific textures visible (e.g., glossy sauce, crispy batter, smooth ceramic, condensation).\")\n",
        "    text_branding: str = Field(..., description=\"Accurate transcription of visible text and detailed description of logos or branding elements.\")\n",
        "    mood_atmosphere: str = Field(..., description=\"Overall feeling conveyed by the image (e.g., cozy, vibrant, elegant, casual).\")\n",
        "    overall_style: str = Field(..., description=\"Characterization of the image style (e.g., photorealistic, cinematic, flat lay, illustration).\")\n",
        "\n",
        "    # Optional: Add a validator to ensure fields are not empty\n",
        "    @field_validator('*', mode='before')\n",
        "    def check_not_empty(cls, value):\n",
        "        if isinstance(value, str) and not value.strip():\n",
        "            return \"(Not specified)\" # Provide a default if empty\n",
        "        return value\n",
        "\n",
        "print(\"✅ Pydantic model 'FnbImageDescription' defined.\")\n",
        "\n",
        "# @title Define Image Handling Function\n",
        "\n",
        "def encode_image_to_base64(image_path: str, max_size=(1024, 1024)) -> str:\n",
        "    \"\"\"Loads an image, resizes if needed, and encodes it to base64.\"\"\"\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            # Convert image to RGB if it's not (e.g., RGBA, P)\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "\n",
        "            # Optional: Resize image to prevent exceeding token limits\n",
        "            # img.thumbnail(max_size) # Uncomment if needed\n",
        "\n",
        "            buffered = BytesIO()\n",
        "            img.save(buffered, format=\"JPEG\") # Save as JPEG\n",
        "            img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "            return img_str\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Error: Image file not found at {image_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error encoding image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"✅ Image encoding function 'encode_image_to_base64' defined.\")\n",
        "\n",
        "# @title Define Prompt Construction Function\n",
        "\n",
        "# Baseline Prompt (as defined previously)\n",
        "BASELINE_PROMPT = \"\"\"\n",
        "Analyze the provided F&B image in meticulous detail. Generate a comprehensive description suitable for recreating this exact image using a text-to-image AI. Describe the following elements:\n",
        "\n",
        "1.  **Primary Subject(s):** Identify and describe the main food, drink, person, or element. Include details like ingredients, preparation style (e.g., grilled, fried, steamed), presentation, specific actions (e.g., pouring, eating).\n",
        "2.  **Composition & Framing:** Describe the layout (e.g., centered, rule of thirds, asymmetrical), camera angle (e.g., eye-level, overhead shot, low angle, Dutch tilt), and framing (e.g., extreme close-up, close-up, medium shot, full shot, wide shot).\n",
        "3.  **Background & Environment:** Detail the setting (e.g., restaurant table, kitchen counter, outdoor picnic, abstract background), surfaces (e.g., wooden table, marble countertop, checkered tablecloth), other objects present (e.g., cutlery, napkins, other dishes, decor), and depth of field (e.g., sharp focus on subject with heavily blurred background, deep focus with everything sharp).\n",
        "4.  **Lighting & Color:** Describe the light source and style (e.g., bright natural daylight from window, warm indoor ambient light, dramatic studio flash, soft diffused light), direction of light, presence and softness of shadows, highlights, dominant color palette, and overall color temperature (e.g., warm tones, cool tones, vibrant, muted).\n",
        "5.  **Texture & Materials:** Mention specific textures visible (e.g., glossy sauce, crispy batter, fluffy bread, smooth ceramic plate, rough wooden board, condensation on glass, metallic sheen of cutlery).\n",
        "6.  **Text & Branding:** Accurately transcribe any visible text (e.g., on packaging, menus, signs). Describe any logos, specific brand colors used prominently, or recognizable branding elements in detail.\n",
        "7.  **Mood & Atmosphere:** Describe the overall feeling conveyed by the image (e.g., cozy and intimate, bright and energetic, rustic and homely, elegant and sophisticated, casual and fun, busy and dynamic).\n",
        "8.  **Overall Style:** Characterize the image style (e.g., photorealistic, cinematic, food photography style, candid shot, flat lay, vector illustration, graphic design with photo elements).\n",
        "\"\"\"\n",
        "\n",
        "# Category-Specific Emphasis (as defined previously)\n",
        "CATEGORY_EMPHASIS = {\n",
        "    \"Product Shot\": \"Emphasis for Product Shot: Pay extra attention to the details of the food/drink item itself – texture, color accuracy, freshness indicators (e.g., steam, droplets), plating details, garnishes, and how the lighting highlights the product's appeal. Describe the dishware/glassware precisely.\",\n",
        "    \"Lifestyle Shot\": \"Emphasis for Lifestyle Shot: Focus on the people involved – their expressions, actions, interactions with the product or each other, clothing style, and body language. Describe how the product is integrated into the scene and the overall narrative suggested (e.g., friends enjoying brunch, family dinner, solo coffee break).\",\n",
        "    \"Menu Displays\": \"Emphasis for Menu Display: Prioritize accurate transcription of all visible text, including item names, descriptions, and prices. Describe the menu's layout, typography (font style, size, weight), color scheme, any graphical elements (lines, boxes, icons), and the material/context if it's a physical menu photo (e.g., chalkboard, printed paper, digital screen). Note the overall readability and design style.\",\n",
        "    \"Promotional Graphics\": \"Emphasis for Promotional Graphic: Accurately transcribe all promotional text (offer details, dates, calls to action). Describe the graphic design elements used (e.g., background color/gradient, shapes, icons, font styles). If it combines photos and graphics, describe how they are integrated. Detail the overall visual hierarchy and intended message.\",\n",
        "    \"Branding Elements\": \"Emphasis for Branding Element: Focus intensely on the specific branding element shown (e.g., logo, packaging detail, unique sign). Describe its colors, shapes, typography, and material precisely. Explain its context within the image and how it contributes to the overall brand identity.\",\n",
        "    \"Location/Ambience Shots\": \"Emphasis for Location/Ambience: Describe the key features of the space – decor style (e.g., modern, rustic, industrial), furniture, lighting fixtures, color scheme, materials (wood, brick, metal), sense of space (cozy, spacious), cleanliness, and overall atmosphere it creates for a customer. Mention specific details like wall art, plants, table settings if visible.\",\n",
        "    \"Event Promotions\": \"Emphasis for Event Promotion: Accurately transcribe all event details (name, date, time, location, description, contact info, price). Describe any specific imagery related to the event theme (e.g., musical instruments, wine bottles, specific food). Detail the overall design style of the flyer/poster/graphic and its call to action.\",\n",
        "    \"Behind-the-Scenes (BTS)\": \"Emphasis for BTS: Describe the action taking place (e.g., cooking, plating, ingredient prep, staff interaction). Detail the environment (e.g., kitchen equipment, staff uniforms, raw ingredients) and the sense of activity or focus. Capture the candid, authentic feel typical of BTS shots.\",\n",
        "    \"Default\": \"\" # For categories not listed or if no emphasis is needed\n",
        "}\n",
        "\n",
        "def construct_prompt(category: Optional[str] = None, use_category_emphasis: bool = False) -> str:\n",
        "    \"\"\"Constructs the prompt, optionally adding category-specific emphasis.\"\"\"\n",
        "    prompt = BASELINE_PROMPT\n",
        "    if use_category_emphasis and category:\n",
        "        emphasis = CATEGORY_EMPHASIS.get(category, CATEGORY_EMPHASIS[\"Default\"])\n",
        "        if emphasis:\n",
        "            prompt += \"\\n\\n\" + emphasis\n",
        "    return prompt\n",
        "\n",
        "print(\"✅ Prompt construction function 'construct_prompt' defined.\")\n",
        "\n",
        "# @title Define Core Inference Function\n",
        "\n",
        "def get_structured_description(\n",
        "    model_name: str,\n",
        "    image_base64: str,\n",
        "    prompt: str\n",
        ") -> Optional[FnbImageDescription]:\n",
        "    \"\"\"Sends image and prompt to a model via OpenRouter and gets a structured description.\"\"\"\n",
        "    try:\n",
        "        print(f\"   Querying {model_name}...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            response_model=FnbImageDescription,\n",
        "            max_retries=1, # Retry once on failure\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": prompt},\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
        "                            },\n",
        "                        },\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=2048, # Adjust as needed\n",
        "            temperature=0.1, # Lower temperature for more deterministic descriptions\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        print(f\"   ✅ Success for {model_name} in {end_time - start_time:.2f} seconds.\")\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Error querying {model_name}: {e}\")\n",
        "        # Consider more specific error handling (e.g., for API errors, validation errors)\n",
        "        return None\n",
        "\n",
        "print(\"✅ Core inference function 'get_structured_description' defined.\")\n",
        "\n",
        "\n",
        "# @title Define Main Processing Workflow\n",
        "\n",
        "# --- Configuration ---\n",
        "# List of OpenRouter model identifiers to test (Update based on Step 1.2 and availability)\n",
        "# Ensure these models support vision input on OpenRouter\n",
        "MODELS_TO_TEST = [\n",
        "    \"openai/gpt-4o\",\n",
        "    \"anthropic/claude-3.7-sonnet-2025XXXX\", # Replace XXXX with actual date string if needed\n",
        "    \"google/gemini-pro-vision\", # Or \"google/gemini-1.5-pro-latest\" if available and preferred\n",
        "    \"meta-llama/llama-3.1-405b-instruct\", # Example, check OpenRouter for Llama 4 / 3.2 vision models\n",
        "    \"qwen/qwen-max-longcontext\", # Example, check OpenRouter for specific Qwen VL models\n",
        "    # Add other models here\n",
        "]\n",
        "\n",
        "# --- Input Data ---\n",
        "# List of images to process. Each item is a tuple: (image_id, image_path, category)\n",
        "# Replace with your actual image paths and categories from Step 1.1\n",
        "IMAGES_TO_PROCESS = [\n",
        "    (\"prod_001\", \"path/to/your/product_shot_1.jpg\", \"Product Shot\"),\n",
        "    (\"life_001\", \"path/to/your/lifestyle_shot_1.png\", \"Lifestyle Shot\"),\n",
        "    (\"menu_001\", \"path/to/your/menu_display_1.jpg\", \"Menu Displays\"),\n",
        "    (\"promo_001\", \"path/to/your/promo_graphic_1.jpeg\", \"Promotional Graphics\"),\n",
        "    # Add all other images from your dataset here...\n",
        "]\n",
        "\n",
        "# --- Workflow Execution ---\n",
        "\n",
        "results_list = []\n",
        "\n",
        "# Use tqdm for progress bar\n",
        "for image_id, image_path, category in tqdm(IMAGES_TO_PROCESS, desc=\"Processing Images\"):\n",
        "    print(f\"\\nProcessing Image: {image_id} ({category}) - {image_path}\")\n",
        "\n",
        "    # 1. Encode Image\n",
        "    image_base64 = encode_image_to_base64(image_path)\n",
        "    if not image_base64:\n",
        "        print(f\"   Skipping image {image_id} due to encoding error.\")\n",
        "        continue\n",
        "\n",
        "    # 2. Construct Prompt (Choose whether to use category emphasis)\n",
        "    # Set use_category_emphasis=True to add specific instructions\n",
        "    use_category_emphasis_flag = False # Or True\n",
        "    prompt_text = construct_prompt(category, use_category_emphasis=use_category_emphasis_flag)\n",
        "\n",
        "    # 3. Iterate through models\n",
        "    for model_name in tqdm(MODELS_TO_TEST, desc=f\"  Models for {image_id}\", leave=False):\n",
        "        description_obj = get_structured_description(model_name, image_base64, prompt_text)\n",
        "\n",
        "        # Store results\n",
        "        result_data = {\n",
        "            \"Image ID\": image_id,\n",
        "            \"Category\": category,\n",
        "            \"Model\": model_name,\n",
        "            \"Prompt Type\": \"Category-Specific\" if use_category_emphasis_flag and category else \"Baseline\",\n",
        "        }\n",
        "\n",
        "        if description_obj:\n",
        "            # Add structured fields to the result dictionary\n",
        "            result_data.update(description_obj.model_dump())\n",
        "            result_data[\"Status\"] = \"Success\"\n",
        "        else:\n",
        "            # Add empty fields if the description failed\n",
        "            for field in FnbImageDescription.model_fields:\n",
        "                 result_data[field] = \"ERROR\"\n",
        "            result_data[\"Status\"] = \"Error\"\n",
        "\n",
        "        results_list.append(result_data)\n",
        "\n",
        "print(\"\\n✅ Workflow finished.\")\n",
        "\n",
        "# @title Display Results in a DataFrame\n",
        "\n",
        "if results_list:\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "\n",
        "    # Set display options for better readability\n",
        "    pd.set_option('display.max_rows', None)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', None)\n",
        "    pd.set_option('display.max_colwidth', 150) # Adjust width as needed\n",
        "\n",
        "    print(\"\\n--- Comparison Results ---\")\n",
        "    # Display the DataFrame - Transpose might be useful for comparing fields across models for ONE image\n",
        "    # For comparing models across images, the standard view is better.\n",
        "    display(results_df)\n",
        "\n",
        "    # Example: To compare descriptions for a specific image ID side-by-side\n",
        "    # specific_image_id = \"prod_001\" # Change to the ID you want to inspect\n",
        "    # display(results_df[results_df[\"Image ID\"] == specific_image_id].set_index('Model').T)\n",
        "\n",
        "    # --- Optional: Save results to CSV ---\n",
        "    # results_df.to_csv(\"fnb_llm_evaluation_results.csv\", index=False)\n",
        "    # print(\"\\n✅ Results saved to fnb_llm_evaluation_results.csv\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo results generated.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m⚠️ OpenRouter API Key not found.\n",
            "Please set the OPENROUTER_API_KEY environment variable or in a .env file.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f85d09388ab1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Patch the OpenAI client to add structured response capabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m client = instructor.patch(\n\u001b[0;32m---> 49\u001b[0;31m     OpenAI(\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://openrouter.ai/api/v1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOPENROUTER_API_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "XcKsmJLaMoms",
        "outputId": "767edd9c-84d9-4275-a48f-492b4bf81364"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "fN6vGrB3QXfI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}